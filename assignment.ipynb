{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ucitavamo podatke i pregledavamo njihovu strukturu i velicinu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  Oh, how the headlines blared:\\nChatbots were T...\n",
      "1  If you’ve ever found yourself looking up the s...\n",
      "2  Machine learning is increasingly moving from h...\n",
      "3  If your understanding of A.I. and Machine Lear...\n",
      "4  Want to learn about applied Artificial Intelli...\n",
      "(337, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Format je tipa string (object), postoji jedna kolona, velicina je 337 zapisa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nakon uvida u strukturu skupa podataka prelazimo na izvrsavanje transformacije podataka:\n",
    "- prvo preuzimamo potrebne *korpuse* i *model*\n",
    "- **tokenizacija** - podjela teksta na dijelove\n",
    "- **stopwords** - uklanjanje zaustavnih rijeci koje nemaju znacajnu vrijednost(veznici,pomocni glagoli itd...)\n",
    "- **lematizacija** - proces skracivanja rijeci za smanjenje šuma i poboljsanje pretrage\n",
    "- **etiketiranje teksta** - dodavaju se oznake ili etikete dijelovima teksta (imenica,glagol itd...)\n",
    "\n",
    "Cijeli proces ce se izvrsiti u jednoj funkciji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [(Oh, UH), (,, ,), (headline, JJ), (blared, VB...\n",
      "1    [(’, NN), (ever, RB), (found, VBD), (looking, ...\n",
      "2    [(Machine, NN), (learning, VBG), (increasingly...\n",
      "3    [(understanding, VBG), (A.I, NNP), (., .), (Ma...\n",
      "4    [(Want, NNP), (learn, NN), (applied, VBD), (Ar...\n",
      "Name: processed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# pozivamo klasu za lematizaciju\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# ucitavanje skupa zaustavnih rijeci\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def transform_data(column):\n",
    "    \n",
    "    tokens = word_tokenize(column)\n",
    "    \n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    pos_tags = pos_tag(lemmatized_tokens)\n",
    "    return pos_tags\n",
    "\n",
    "# pozivamo funkciju transform\n",
    "# pomocu funkcije apply() da se primjeni na cijeloj koloni\n",
    "data['processed'] = data['text'].apply(transform_data)\n",
    "\n",
    "print(data['processed'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobili smo transformiranu kolonu sa tokenima i njihovim oznakama/etiketama.\n",
    "\n",
    "Prelazimo na dobijanje *vektora znacenja* - proces *vektorizacije* (pretvaranje teksta u numericki oblik) sa mnogo manje dimenzija i ocuvanja semantickog znacenja rijeci.\n",
    "\n",
    "Koristit cemo **word2vec** algoritam iz biblioteke *gensim*\n",
    "\n",
    "Prije svega kreiramo listu i prolazimo kroz 'processed' kolonu.Svaki artikl kolone je lista torki koja sadrzi rijec i odredenu oznaku.U ugniježđenoj listi prolazimo kroz svaku torku, ubacujemo rijec i izbacujemo oznaku zato sto **Word2Vec** model uci kontekstu u kojem se rijeci pojavljuju. Unos oznaka/etiketa zakompliciralo bi proces treniranja modela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Za pronalazak dvije najslicnije rijeci i dvije sa najmanje slicnim znacenjem koristimo svojstvo **wv** i metoda **most_similar()**\n",
    "- nakon treiranja modela kreira se objekt **wv** koji sadrzi mapiranja stvarnih rijeci i njihovih vektora znacenja.\n",
    "- **most_similar** metoda za pronalazenje slicnih rijeci - prosljeduje se zadana rijec i vrijednost parametra **topn** koji definira koliko slicnih rijeci cemo dobiti, u ovom slucaju je dvije rijeci. Za suprotnost zadane rijeci koristimo **negative** parametar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Najslicnija rijec zadanoj rijeci 'android': [('mistrustful', 0.8223909735679626), ('souvent', 0.8174886107444763)]\n",
      "Najmanje slica rijec zadanoj rijeci 'android': [('restate', 0.5823929309844971), ('3d-geometry', 0.5622067451477051)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "filtrirane_rijeci = [[word for word, tag in article] for article in data['processed']]\n",
    "# print(rijeci[0])\n",
    "\n",
    "# treniramo model\n",
    "model = Word2Vec(filtrirane_rijeci, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# zadana rijec \n",
    "rijec = 'android' \n",
    "most_similar = model.wv.most_similar(rijec, topn=2)\n",
    "least_similar = model.wv.most_similar(negative=[rijec], topn=2)\n",
    "\n",
    "\n",
    "print(\"Najslicnija rijec zadanoj rijeci '{}':\".format(rijec), most_similar)\n",
    "print(\"Najmanje slica rijec zadanoj rijeci '{}':\".format(rijec), least_similar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
